---
title: Mise en production des projets de data science
subtitle: |
  ENSAE 3A - 2023/2024
author:
    - Romain Avouac
    - Lino Galiana
# date: 
slide-number: true
footer: |
  Bonnes pratiques pour la mise en production des projets de data science
# uncomment for French presentations:
lang: fr-FR
# for blind readers:
slide-tone: false
# for @olevitt:
chalkboard: # press the B key to toggle chalkboard
  theme: whiteboard
# uncomment to use the multiplex mode:
#multiplex: true
format:
  # pick the light mode (onyxia-revealjs) or the dark mode (onyxia-dark-revealjs)
  onyxia-revealjs:
  #onyxia-dark-revealjs:
    incremental: true 
    output-file: index.html
controls: true
css: custom.css
from: markdown+emoji
---

# Introduction

## Contexte

- [**Qui sommes-nous ?**]{.orange}
    - des [**data scientists**]{.blue2} de l'Insee
    - frustrés par l'[**approche**]{.blue2} souvent purement [**technique**]{.blue2} de la data science
    - convaincus que les [**bonnes pratiques**]{.blue2} valent à être enseignées

- <romain.avouac@insee.fr>, <lino.galiana@insee.fr>

## Qu'est ce qu'un data scientist ?

![](img/sexiest-job.png){fig-align="center" height=250}

- Tendance à la [**spécialisation**]{.orange} : *data analyst*, *data engineer*, *ML Engineer*...

- Rôle d'[**interface**]{.orange} entre métier et équipes techniques
    - [**Compétences mixtes**]{.blue2} : savoir métier, modélisation, IT

## La notion de mise en production

- [**Mettre en production**]{.orange} : faire [**vivre**]{.blue2} une application dans l'espace de ses [**utilisateurs**]{.blue2}
    - Notion simple mais mise en oeuvre compliquée !

- [**Dépasser le stade de l'expérimentation**]{.orange}
    - Comprendre les [**besoins**]{.blue2} des utilisateurs
    - [**Bonnes pratiques**]{.blue2} de développement
    - Techniques informatiques d'[**industrialisation**]{.blue2}

## La notion de bonnes pratiques

- [**Origine**]{.blue2} : communauté des développeurs logiciels

- [**Constats**]{.blue2} :
    - le [_"code est plus souvent lu qu'écrit"_]{.green2} ([Guido Van Rossum](https://fr.wikipedia.org/wiki/Guido_van_Rossum))
    - la maintenance d'un code est très coûteuse

- [**Conséquence**]{.blue2} : un ensemble de [**règles informelles**]{.orange}, conventionnellement acceptées comme produisant des logiciels [**fiables**]{.orange}, [**évolutifs**]{.orange} et [**maintenables**]{.orange}

## Pourquoi s'intéresser aux bonnes pratiques ?

<br>

L'activité du *datascientist* tend à se rapprocher de celle du développeur :

- projets [**intenses en code**]{.blue2}

- [**projets collaboratifs**]{.blue2} et de grande envergure

- [**complexification**]{.blue2} des données et des [**infrastructures**]{.blue2}

- [**déploiement**]{.blue2} d'applications pour valoriser les modèles

## Contenu du cours {.scrollable}

- [**Pré-requis**]{.orange}
    - Introduction au terminal `Linux`
    - [**Contrôle de version**]{.blue2} avec `Git`

- [**Bonnes pratiques**]{.orange} de développement
    - [**Travail collaboratif**]{.blue2} avec `Git`
    - [**Qualité**]{.blue2} du code
    - [**Structure**]{.blue2} des projets

- [**Mise en production**]{.orange}
    - Maximiser la [**portabilité**]{.blue2}
    - [**Déployer**]{.blue2} et [**valoriser**]{.blue2} un projet de data science
    - [**MLOps**]{.blue2}

## Site web du cours

- [https://ensae-reproductibilite.github.io/website/](https://ensae-reproductibilite.github.io/website/)

- Tout le contenu du cours est en [***open-source***]{.orange}
    - [GitHub](https://github.com/orgs/ensae-reproductibilite/repositories) {{< fa brands github >}}
    - *Pull Requests* -> bonus {{< fa comment-dollar >}}

## Modalités pédagogiques

- Apprentissage par la pratique
    - [Application](https://ensae-reproductibilite.github.io/website/chapters/application.html) : industrialisation d'un projet de ML

- Langage : `Python` {{< fa brands python >}}
    - Langage [**dominant**]{.blue2} dans le monde de la donnée
    - Les principes présentés sont [**agnostiques**]{.blue2} au langage

- Environnement d'exécution : [SSP Cloud](https://datalab.sspcloud.fr/)
    - Environnement de développement [**normalisé**]{.blue2}
    - Véritable environnement de [**production**]{.blue2}
    - Acquisition des [**bonnes pratiques**]{.blue2}

## Evaluation

- [**Objectif**]{.orange} : mise en pratique [**réaliste**]{.orange} des notions étudiées
    - [**Problématique métier**]{.blue2}
    - [**Données réelles**]{.blue2}

- Evaluation en [**deux parties**]{.orange} :
    - [**En groupe**]{.blue2} : [**projet**]{.blue2} à construire selon 3 "parcours"
    - [**Individuel**]{.blue2} : [**revue de code**]{.blue2} d'un autre projet








# Le travail collaboratif avec `Git`

## Pourquoi utiliser Git ?

![](img/timeline.png){fig-align="center" height=475}

[Source](https://thinkr.fr/travailler-avec-git-via-rstudio-et-versionner-son-code/)

## Concepts essentiels

![](img/gitallinone.png){height="400" fig-align="center"}

[Source](http://fabacademy.org/2021/labs/bhubaneswar/students/deepak-chaudhry/ia_PPFP.html)

## Bonnes pratiques {auto-animate=true .smaller}

__Que versionne-t-on ?__

- Essentiellement du [**code source**]{.orange}
- [__Pas d'outputs__]{.orange} (fichiers `.html`, `.pdf`, modèles...)
- [__Pas de données__]{.orange}, d'informations locales ou sensibles

. . .

:::{.callout-note}

Pour définir des règles qui évitent de committer tel ou tel fichier, on utilise
un fichier nommé __`.gitignore`__.

Si on mélange du code et des éléments
annexes (_output_, données...) dans un même dossier, il [__faut consacrer du temps à ce fichier__]{.orange}.

Le site [`gitignore.io`](https://www.toptal.com/developers/gitignore) peut vous fournir
des modèles.

N'hésitez pas à y ajouter des règles conservatrices (par exemple `*.csv`), 
comme cela est expliqué dans [la documentation `utilitR`](https://www.book.utilitr.org/git.html?q=gitignore#gitignore).

:::

## Bonnes pratiques {auto-animate=true .smaller}

__Format des commits__

::: {layout="[40,60]"}

- [**Fréquence**]{.orange}
    - Aussi souvent que possible
    - Le lot de modifications doit "faire sens"
- [**Messages**]{.orange}
    - Courts et informatifs (comme un titre de mail)
    - Décrire [**le pourquoi plutôt que le comment**]{.orange} dans le texte

![](img/titre-commit.png)

:::

## Outils pour le travail collaboratif

- L'éco-système `Git` [**facilite** le travail collaboratif]{.blue2}
    - `Git` {{< fa brands git-alt >}} : modèle des [__branches__]{.orange}
    - `GitHub` {{< fa brands github >}} / `GitLab` {{< fa brands gitlab >}} : [**Issues**, **Pull Requests**, **Forks**]{.orange}

## Le modèle des branches

::: {layout="[[-1], [1], [-1]]"}
![](img/branches.png){fig-align="center" height=350}
:::

## Les outils de contribution

- [***Issue***]{.orange} : soumettre un [**problème**]{.blue2} ou une [**suggestion**]{.blue2} aux développeurs d'un projet

- [***Pull Request***]{.orange} : proposer aux développeurs d'un projet d'[**intégrer des modifications**]{.blue2}

- [***Fork***]{.orange} : faire la [**copie**]{.blue2} d'un projet existant dans son espace personnel
    - Indispensable pour faire une *pull request* à un dépôt sur lequel on n'a pas les droits 

## Une organisation courante : le *GitHub flow*

![](img/ghflow.png)

Description plus détaillée : [ici](https://docs.github.com/en/get-started/quickstart/github-flow)







# Qualité du code

## Enjeux

- D'une vision utilitariste du code à une vision du code comme [**outil de communication**]{.orange}

- Favoriser la [**lisibilité**]{.orange} et la [**maintenabilité**]{.orange}

- Faciliter la [**réutilisation**]{.orange}

## Principes généraux

- Adopter les [**standards communautaires**]{.orange}

- Utiliser des [**fonctions**]{.orange}

- [**(Auto-)documenter**]{.orange} son code

## :one: Standards communautaires

> *"Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread"*
>
> [Tidyverse Style Guide (R)](https://style.tidyverse.org/)

- [**Python**]{.blue2} : [PEP8](https://peps.python.org/pep-0008/), [PEP 257](https://peps.python.org/pep-0257/)
    - Des règles *opinionated*, mais [**conventionnelles**]{.blue2}

- La [**cohérence intra-projet**]{.orange} doit toujours primer

## :one: Standards communautaires - Outils {.smaller}

- [**Linters**]{.blue2} : diagnostic de qualité du code
    - [Pylint](https://github.com/PyCQA/pylint)

- [**Formatters**]{.blue2} : application automatique des standards
    - [Black](https://github.com/psf/black)

. . .

::: {.callout-tip}
- [Exemples d’erreurs repérées]{.blue2} par un _linter_ : 
    + lignes de code trop longues ou mal indentées, parenthèses non équilibrées, noms de fonctions mal construits…
- [Exemples d’erreurs __non__ repérées]{.blue2} par un _linter_ :
    + fonctions mal utilisées, arguments mal spécifiés, structure du code incohérente, code insuffisamment documenté…
:::

## :two: Utiliser des fonctions {.smaller}

::: {.callout-important}
## Règle d'or

Il faut utiliser une [**fonction**]{.red2} dès qu'on utilise une même
portion de code plus de deux fois ([**_don't repeat yourself_ (DRY)**]{.red2})
:::

- [**Limite les risques d'erreurs**]{.orange} liés aux copier/coller
- Rend le code [**plus lisible**]{.orange} et [**plus compact**]{.orange}
- [**Un seul endroit**]{.orange} du code à modifier lorsqu'on souhaite modifier le traitement
- Facilite la [**réutilisation**]{.orange} et la [**documentation**]{.orange} du code !

. . .

::: {.callout-tip}
## Règles pour écrire des fonctions **pertinentes**

- Une tâche = une fonction
- Une tâche complexe = un enchaînement de fonctions réalisant chacune une tâche simple
- Limiter l'utilisation de variables globales.

:::

## :three: Documenter son code

- Documenter le [__pourquoi__]{.orange} plutôt que le [__comment__]{.orange}

- Privilégier l'[**auto-documentation**]{.orange} via des nommages pertinents

. . .

::: {.callout-tip}
## Comment bien documenter un script ?

- [**Minimum**]{.orange} 🚦 : décrire ce que le code fait au début du script
- [**Bien**]{.orange} 👍 : commenter les parties "délicates" du code
- [**Idéal**]{.orange} 💪 : documenter ses fonctions avec des *docstrings*
:::





# Structure des projets

## Enjeux

- Favoriser la [**lisibilité**]{.orange} et la [**maintenabilité**]{.orange}

- Enjeux spécifiques à la data science
    - [**Expérimentation**]{.blue2}
    - [**Non-linéarité**]{.blue2}
    - [**Reproductibilité**]{.blue2}

## Principes généraux

- Favoriser une [**structure modulaire**]{.blue2} selon l'état du projet
    - [**Exploration**]{.orange} : travail à partir de [**notebooks**]{.blue2}
    - [**Industrialisation**]{.orange} : adopter une structure type [**package**]{.blue2}

- Adopter les [**standards communautaires**]{.orange}

- [**(Auto-)documenter**]{.orange} son projet

## :one: Phase d'exploration : *notebooks*

- [**Avantages**]{.orange}
  - [**Interactivité**]{.blue2} : idéal pour l'expérimentation
  - [**Communication**]{.blue2} : diffusion de résultats sous forme exécutable

- [**Inconvénients**]{.orange}
  - [**Reproductibilité**]{.blue2} généralement limitée
  - Pas adaptés pour l'[**automatisation**]{.blue2}
  - Mal gérés par le [**contrôle de version**]{.blue2}

## :two: Industrialisation : structure modulaire

- [**Premier niveau**]{.orange} : structuration du [**code**]{.orange}

- Adopter une structure type [**package**]{.orange}
  - Des [**fonctions**]{.blue2} rangées dans des [**modules**]{.blue2}
  - Un script principal (`main`) [**orchestre**]{.blue2} les traitements
  - Utilisation de [**chemins relatifs**]{.blue2} uniquement

## :two: Industrialisation : structure modulaire

- [**Deuxième niveau**]{.orange} : structuration du [**projet**]{.orange}

. . .

![](img/project-modularity.png){fig-align="center" height=350}

## :three: Adopter les standards communautaires

- [**Templates**]{.orange} de projets : [**Cookiecutters**]{.blue2}
    - [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/)
    - [Cookiecutter Python Package](https://py-pkgs.org/03-how-to-package-a-python#creating-a-package-structure)

- La [**cohérence intra-projet**]{.orange} doit toujours primer

## :four: Documenter son projet

- Favoriser l'[**auto-documentation**]{.orange} via des nommages pertinents

## L'auto-documentation : illustration

```
├── report.ipynb
├── correlation.png
├── data.csv
├── data2.csv
├── fig1.png
├── figure 2 (copy).png
├── report.pdf
├── partial data.csv
├── script.py
└── script_final.py
```

- Difficile de rentrer dans le projet...
    - Tout au [**même niveau**]{.blue2}
    - Titres [**non informatifs**]{.blue2}

## L'auto-documentation : illustration 

```
├── data
│   ├── raw
│   │   ├── data.csv
│   │   └── data2.csv
│   └── interim
│       └── partial data.csv
├── notebooks
│   └── report.ipynb
├── src
|   ├── script.py
│   └── script_final.py
└── reports
    ├── report.pdf
    └── figures
        ├── fig1.png
        ├── figure 2 (copy).png
        ├── figure10.png
        └── correlation.png
```

- Une structure déjà plus lisible !
    - Les titres restent [**non informatifs**]{.blue2}

## L'auto-documentation : illustration 

```
├── data
│   ├── raw
│   │   ├── dpe_logement_202103.csv
│   │   └── dpe_logement_202003.csv
│   └── interim
│       └── dpe_logement_merged_preprocessed.csv
├── notebooks
│   └── report.ipynb
├── src
|   ├── main.R
|   ├── preprocessing.R
│   └── generate_plots.R
└── reports
    ├── report.pdf
    └── figures
        ├── histogram_energy_diagnostic.png
        ├── barplot_consumption_pcs.png
        ├── correlation_matrix.png
        └── correlation.png
```

- Une structure [**auto-documentée**]{.blue2}
    - On comprend le projet sans même lire le code

## :four: Documenter son projet

- Favoriser l'[**auto-documentation**]{.orange} via des nommages pertinents

- Inclure un fichier `README.md` à la racine du projet
    - [**Carte d'identité**]{.blue2} et [**vitrine**]{.blue2} du projet
    - Présente le [**contexte**]{.blue2} et le [**fonctionnement**]{.blue2} du projet

- Si [**open-source**]{.orange} : inclure une [licence](https://docs.github.com/fr/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository)






# Application

## Bonnes pratiques de développement

- Consignes sur le [site du cours](https://ensae-reproductibilite.github.io/website/chapters/application.html)
    - Partie :zero: : initialisation de l'environnement et du projet
    - Partie :one: : qualité du code
    - Partie :two: : adoption d'une structure modulaire






# Stockage des données

## Enjeux

- [**Massification**]{.orange} des données

![](img/vvv.png){fig-align="center" height=350}

[Source](https://www.packtpub.com/product/artificial-intelligence-with-python-second-edition/9781839219535)

## Choisir des technologies adaptées

- :one: [**Infrastructure**]{.orange}

- :two: [**Formats**]{.orange} de données

- :three: [***Frameworks***]{.orange} de traitement de données

## :one: Le *data lake*

- Un stockage [**peu coûteux**]{.orange} fait pour des [**données**]{.orange}
  - [**volumineuses**]{.blue2}
  - [**brutes**]{.blue2}
  - issues de [**sources variées**]{.blue2}

. . .

![](img/datalake.png){height="400" fig-align="center"}

## :one: Le stockage objet

- Standard des *data lakes* dans le *cloud*
    - Implémentation de référence : [Amazon S3](https://aws.amazon.com/fr/s3/)
    - Implémentation open-source : [MinIO](https://min.io/)

. . .

![](img/minio.svg){height="400" fig-align="center"}

## :two: Formats de données

::: {.incremental}
- Le choix d'un format de données répond à un [**arbitrage**]{.orange} entre plusieurs critères :
    - [**Public cible**]{.blue2}
    - [**Finalité**]{.blue2} (traitement, analyse, diffusion)
    - [**Volumétrie**]{.blue2}
    - [**Interopérabilité**]{.blue2}
:::

## :two: Limites des formats usuels

- Les [**formats usuels**]{.orange} (`CSV`, `JSON`, `XML`) sont utiles pour :
    - Le traitement de [**faibles volumes**]{.blue2} de données
    - La [**diffusion**]{.orange} de données

- [**Mal adaptés**]{.orange} au traitement de [**données volumineuses**]{.orange}
  - [**Non-compressés**]{.blue2} : espace disque élevé
  - [**Orientés ligne**]{.blue2} : un traitement sur une/des colonne(s) implique une lecture complète du fichier

## :two: Orientation ligne vs. orientation colonne

![](img/row-column.png){fig-align="center"}

## :two: Formats prometteurs

- [**Propriétés souhaitées**]{.orange} 
  - Orientés [**colonne**]{.blue2}
  - Efficients en [**stockage disque**]{.blue2}
  - Efficients en [**RAM**]{.blue2}
  - [**Interopérables**]{.blue2}

- Formats [**candidats**]{.orange} 
  - `Parquet` : [**stockage disque**]{.blue2}
  - `Arrow` : traitement en [**mémoire**]{.blue2}

## :two: `Parquet` : propriétés

- [**Orienté colonne**]{.orange}
  - Adapté aux traitements [**analytiques**]{.blue2}
  - Conçu pour être écrit une fois mais lu fréquemment

- [**Optimisé**]{.orange}
  - [**Compression**]{.blue2} (jusqu'à 87 % moins d'espace qu'un CSV)
  - Parcours du fichier (jusqu'à 34x plus rapide qu'un CSV)

- [**Interopérable**]{.blue2} et open-source

## :two: `Parquet` : partitionnement

- [**Division en blocs**]{.orange} des données selon un [**critère**]{.orange}
  - [**Optimise la lecture**]{.blue2} pour certaines *queries*

![](img/partitions.png){fig-align="center"}

## :three: Traitement en mémoire

- `Parquet` ne résout pas tout
  - L'espace disque est optimisé
  - Les données décompressées doivent [**passer en RAM**]{.blue2}

## :three: `Arrow`

- Format de données [**en mémoire, orienté colonne**]{.orange}
  - Optimisé pour les traitements [**analytiques**]{.blue2}
  - Pas nécessaire de charger toutes les données en RAM
  - [**Interopérable**]{.blue2}

. . .

![](img/arrow.png){fig-align="center"}

## :three: Traitement des données massives

- `Arrow` ne résout pas tout
  - Les données massives nécessitent des [**infrastructures ***big data***]{.blue2}
  - Les [**algorithmes**]{.blue2} aussi doivent être [**adaptés**]{.blue2}

- Solution : [**calcul distribué**]{.orange}

## :three: `Hadoop MapReduce`

- Développé par Google (2004)

- Popularisé par l'implémentation open-source d'[**Hadoop**]{.orange}

. . .

![](img/mapreduce.png){fig-align="center"}

## :three: `Apache Spark`

- Démocratisation du [**calcul distribué**]{.orange}
  - [**Abstraction**]{.blue2} des opérations MapReduce
  - [**Vitesse d'exécution**]{.blue2} (RAM vs. disque)

. . .

![](img/spark.png){fig-align="center"}

## En résumé

- Utiliser un [**format**]{.orange} de données adapté (`Parquet`)

- Utiliser des [**outils**]{.orange} informatiques adaptés
  - Données [**volumineuses**]{.blue2} : calcul [**en mémoire**]{.blue2} optimisé  (`Arrow` / `DuckDB`)
  - Données [**massives**]{.blue2} : calcul [**distribué**]{.blue2} (`Spark`)







# Portabilité

## "It works... on my machine"

- On a construit un projet [**lisible**]{.orange}, [**structuré**]{.orange} et [**versionné**]{.orange}

- Peut-on [**partager**]{.orange} notre projet ?
    - En théorie, oui !
    - En pratique, c'est toujours plus compliqué...

. . .

![](img/IWOMM.jpg){fig-align="center" height=350}

## Le critère de la portabilité

- Un code ne vit jamais dans une bulle isolée, il contient en général de nombreuses [**adhérences**]{.orange}
    - Des [**dépendances**]{.blue2}
    - Des [**librairies système**]{.blue2}

- Un code est [**portable**]{.orange} s'il peut être exécuté dans un environnement différent que celui du développement

## Limites du mode de travail usuel

- *Workflow* [**classique**]{.orange}
    - Installer une distribution de `Python` sur son poste
    - Développer un projet en installant les packages nécessaires
    - Passer au projet suivant et ainsi de suite

- Quels [**problèmes**]{.orange} peuvent se poser ?

## Limites du mode de travail usuel

- [**Conflits de version**]{.orange} : différents projets peuvent requérir des versions différentes d'un même *package*

- [**Version de `Python` fixe**]{.orange}, celle de l'installation système

- [**Reproductibilité limitée**]{.orange} : difficile de dire quel projet nécessite quel package

- [**Portabilité limitée**]{.orange} : difficile de fixer dans un fichier les dépendances spécifiques à un projet

## Comment favoriser la portabilité ?

- Enjeu central pour la [**mise en production**]{.orange}
    - Passer d'un [**environnement de développement**]{.blue2} à un [**environnement de production**]{.blue2}

- Besoin de [**nouveaux outils**]{.orange}
    - Les [**environnements virtuels**]{.blue2}
    - Les [**conteneurs**]{.blue2}

## Environnements virtuels : fonctionnement

- [**Dossier auto-suffisant**]{.orange} qui :
    - contient un [**intepréteur**]{.blue2} `Python` et des [**packages**]{.blue2}
    - est [**isolé**]{.orange} des autres environnements existants

. . .

![](img/venv.png){fig-align="center" height=350}

[Source](https://www.dataquest.io/blog/a-complete-guide-to-python-virtual-environments/)

## Environnements virtuels : implémentations

- Implémentation standard : [venv](https://docs.python.org/fr/3/library/venv.html)

- Une implémentation populaire en data science : [conda](https://docs.conda.io/en/latest/)
    - Également un *package manager* (comme [pip](https://pip.pypa.io/en/stable/getting-started/), mais multi-langages)

- D'autres implémentations existent : [virtualenv](https://virtualenv.pypa.io/en/latest/), [pyenv](https://github.com/pyenv/pyenv)...

## `venv` : utilisation

- `venv` fait partie de la librairie standard de `Python`

- Utilisation basique (sous `Linux`)
    - [**Créer**]{.orange} un environnement : `python -m venv myenv`
    - [**Activer**]{.orange} l'environnement : `source myenv/bin/activate`
    - [**Installer**]{.orange} des packages : `pip install scikit-learn`
    - [**Quitter**]{.orange} l'environnement : `deactivate`

## Spécifier les dépendances

- Développer dans un [**environnement virtuel**]{.orange} favorise :
    - la [**reproductibilité**]{.blue2} : fixer les packages utilisés et leurs versions
    - la [**portabilité**]{.blue2} : distribuer ces spécifications

- [**Convention**]{.orange} : fichier `requirements.txt` à la [**racine**]{.blue2} du projet (à *commit* !)
    - [**Génération**]{.blue2} : `pip freeze > requirements.txt`
    - [**Installation**]{.blue2} : `pip install -r requirements.txt`

## Le fichier `requirements.txt`

```python
beautifulsoup4==4.12.3
expecttest!=0.2.0
networkx>=3.0.0
numpy
pandas
```

- [**Arbitrage**]{.orange} à trouver entre :
    - [**Reproductibilité**]{.blue2} : [**spécifier**]{.blue2} finement les versions
    - [**Sécurité**]{.blue2} : laisser les versions [**évoluer**]{.blue2}

## Environnements virtuels : limites

- [**Reproductibilité**]{.orange} :
    - [**Version de `Python`**]{.blue2} non-gérée
    - [**Librairies système**]{.blue2} non-gérées

- Peu adaptés aux environnements de [**production**]{.orange} :
    - Reproductibilité limitée -> [**portabilité limitée**]{.blue2}
    - [**Lourdeur**]{.blue2} de gestion des environnements

## Le *gold-standard* de la portabilité

- [**Idée**]{.orange} : au lieu de distribuer la recette pour recréer la bonne machine, peut-on [**distribuer directement la bonne machine**]{.orange} ?
    - On ne peut pas distribuer des [**machines physiques**]{.blue2}
    - Les [**machines virtuelles**]{.blue2} sont [**coûteuses**]{.blue2} à redistribuer

- Les [**conteneurs**]{.orange} offrent le compromis idéal

## Conteneurs vs. machines virtuelles

![](img/docker-vm.png)

Source : [docker.com](https://www.docker.com/resources/what-container/)

## Conteneurs : implémentations

- Plusieurs implémentations des conteneurs
    - [**`Docker`**]{.blue2} est largement prédominant

. . .

![](img/docker.png){fig-align="center" height=200}

## `Docker` : installation

- `Docker` : outil en ligne de commande (CLI)
    - [Instructions](https://docs.docker.com/get-docker/) selon le système d'exploitation
    - Environnement "bac à sable" : [Play with Docker](https://labs.play-with-docker.com/)

. . .

![](img/playwithdocker.png){fig-align="center" height=350}

## Le `Dockerfile`

- Exemple : [conteneurisation d'une API avec FastAPI](https://fastapi.tiangolo.com/deployment/docker/)

. . .

```Dockerfile
# Image Docker de base
FROM python:3.11

# Définition du répertoire de travail
WORKDIR /code

# Copie des fichiers nécessaires sur l'image
COPY requirements.txt /code/requirements.txt

# Installation des dépendances
RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt && \
    python -m spacy download en_core_web_sm

COPY app/ code/app

# Commande lancée par l'image au runtime
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "80", "--proxy-headers"]
```

## `Docker` : fonctionnement

![](img/docker-workflow.png){fig-align="center"}

Source : [k21academy.com](https://k21academy.com/docker-kubernetes/docker-and-kubernetes/)

## `Docker` en pratique

- Présentation détaillée sur la [page du cours](https://ensae-reproductibilite.github.io/website/chapters/portability.html#les-conteneurs)
    - [**Concepts**]{.blue2} (*caching*, *buildtime/runtime*)
    - [**Commandes**]{.blue2} essentielles
    - [**Application**]{.blue2} à un exemple concret

# Application

## Portabilité du projet

- Consignes sur le [site du cours](https://ensae-reproductibilite.github.io/website/chapters/application.html)
    - Partie :three: : construction d’un projet [**portable**]{.orange} et [**reproductible**]{.orange}
        - Construire un [**environnement virtuel**]{.blue2} pour le projet
        - [**Conteneuriser**]{.blue2} l'application avec `Docker`






# Valorisation et déploiement

## Motivation

- La [**massification**]{.orange} et la [**diversification**]{.orange} des données apportent de nombreux changements
    - Le *Data Lake*
    - Le *Data Lab*
    - De nouveaux profils : *Data Scientist*, *Data Engineer*, *Data Architect*

- La majorité des projets de data science [**ne sont pas déployés**]{.orange}

- Besoin d'[**industrialisation**]{.orange} qui nécessite de nouveaux outils

## L'approche DevOps

- Idée : [**unifier**]{.orange} le développement (*dev*) et l'administration système (*ops*)

- But : raccourcir le temps de développement en [**déployant en continu**]{.orange} tout en maintenant la qualité

. . .

![](img/devops.png){fig-align="center" height=300}

## DevOps, DataOps, MLOps ?

- Le [**DevOps**]{.orange} n'intègre pas les spécificités liées à la data science

- [**DataOps**]{.orange} : déploiement et maintenance de [**pipelines de données**]{.blue2}

- [**MLOps**]{.orange} : déploiement et maintenance de [**modèles de Machine Learning**]{.blue2}

- Les [**bonnes pratiques**]{.orange} favorisent la collaboration et facilitent les déploiements

## La mise en production

- On a construit un projet de data science [**reproductible**]{.orange} et conforme aux [**standards**]{.orange} des bonnes pratiques

- Pour [**valoriser**]{.orange} le projet, il faut le [**déployer**]{.orange} dans un environnement en lien avec les utilisateurs
    - Quel est le [**format**]{.blue2} adapté pour le valoriser ?
    - Quelle [**infrastructure de production**]{.blue2} ?
    - Comment [**automatiser**]{.blue2} le processus de déploiement ?

## Format de valorisation

- [**Critères**]{.orange} à prendre en compte :
    - Quels sont les [**utilisateurs**]{.blue2} potentiels ?
    - Seulement de la [**mise à disposition**]{.blue2}, ou besoin d'[**interactivité**]{.blue2} ?
    - Spécificités ML : entraînement en [**batch**]{.blue2} ou [**online**]{.blue2} ?
    - Besoin de [**scalabilité**]{.blue2} ?

- [**Formats usuels**]{.orange} : API, application web, dashboard, site internet, rapport automatisé...

## Exposer un modèle via une API REST

- [**API** : **interface**]{.orange} entre l'utilisateur (client) et le modèle entraîné

- [**API REST**]{.orange} : permet de requêter le modèle avec une syntaxe simple (HTTP) et de manière *scalable*

. . .

![](img/api-docker.png){fig-align="center"}

## Environnement de production

- Dépend essentiellement de l'infrastructure à disposition

- Un [**orchestrateur**]{.orange} de conteneurs répond à plusieurs besoins :
    - Adapter les ressources ([**scaler**]{.blue2}) selon les besoins
    - [**Monitoring**]{.blue2} de l'état de santé des applications
    - Déploiements [**reproductibles**]{.blue2} et [**automatisés**]{.blue2}

. . .

![](img/kubernetes-logo.png){fig-align="center"}

## Fonctionnement de Kubernetes

![](img/kubernetes-archi.png){fig-align="center"}

## L'approche CI/CD : principes

- [**Intégration continue**]{.orange} (CI) : à chaque modification du code source, l'application est automatiquement [**tested, built and released**]{.blue2}

- [**Déploiement continu**]{.orange} (CD) : les nouvelles [**releases**]{.blue2} sont automatiquement déployées

- [**GitOps**]{.orange} : le processus est décrit sous formes de [**manifestes**]{.blue2}, stockés sur un dépôt `Git`

## L'approche CI/CD : exemple

![](img/ci-cd.png){fig-align="center"}

## Pipeline complet

- Les données d'entrée ne sont pas fixes, il faut les intégrer dans un [**pipeline**]{.orange} complet de données

- La représentation est faite sous forme de graphes acycliques dirigés ([**DAG**]{.orange})

. . .

![](img/pipeline.png){fig-align="center"}

## Conclusion

- On a construit un pipeline [**reproductible**]{.orange}, [**automatisé**]{.orange} et [**scalable**]{.orange}

. . .

![](img/devops.png){fig-align="center" height=300}





# MLOps

